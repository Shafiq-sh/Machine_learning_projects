{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eiDWcM_MC3H"
   },
   "source": [
    "# <font color='Violet'>Implementing Logistic regression using Stochastic Gradient Descent without using sklearn</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fk5DSPCLxqT-"
   },
   "source": [
    "<font color='red'> Importing packages</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "42Et8BKIxnsp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpSk3WQBx7TQ"
   },
   "source": [
    "<font color='red'>Creating custom dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "BsMp0oWzx6dv"
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "L8W2fg1cyGdX",
    "outputId": "029d4c84-03b2-4143-a04c-34ff49c88890"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 15), (50000,))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x99RWCgpqNHw"
   },
   "source": [
    "<font color='red'>Splitting data into train and test </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "0Kh4dBfVyJMP"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "gONY1YiDq7jD"
   },
   "outputs": [],
   "source": [
    "# Standardizing the data.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "0DR_YMBsyOci",
    "outputId": "732014d9-1731-4d3f-918f-a9f5255ee149"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500, 15), (37500,), (12500, 15), (12500,))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BW4OHswfqjHR"
   },
   "source": [
    "# <font color='red' size=5>SGD classifier using Sklearn</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "id": "3HpvTwDHyQQy",
    "outputId": "5729f08c-079a-4b17-bf51-f9aeb5abb13b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpha : float\n",
    "# Constant that multiplies the regularization term. \n",
    "\n",
    "# eta0 : double\n",
    "# The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules.\n",
    "\n",
    "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf\n",
    "# Please check this documentation (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "YYaVyQ2lyXcr",
    "outputId": "dc0bf840-b37e-4552-e513-84b64f6c64c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.70, NNZs: 15, Bias: -0.501317, T: 37500, Avg. loss: 0.552526\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.04, NNZs: 15, Bias: -0.752393, T: 75000, Avg. loss: 0.448021\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.26, NNZs: 15, Bias: -0.902742, T: 112500, Avg. loss: 0.415724\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.43, NNZs: 15, Bias: -1.003816, T: 150000, Avg. loss: 0.400895\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.55, NNZs: 15, Bias: -1.076296, T: 187500, Avg. loss: 0.392879\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.65, NNZs: 15, Bias: -1.131077, T: 225000, Avg. loss: 0.388094\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.73, NNZs: 15, Bias: -1.171791, T: 262500, Avg. loss: 0.385077\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.80, NNZs: 15, Bias: -1.203840, T: 300000, Avg. loss: 0.383074\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.86, NNZs: 15, Bias: -1.229563, T: 337500, Avg. loss: 0.381703\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.90, NNZs: 15, Bias: -1.251245, T: 375000, Avg. loss: 0.380763\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.94, NNZs: 15, Bias: -1.269044, T: 412500, Avg. loss: 0.380084\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.98, NNZs: 15, Bias: -1.282485, T: 450000, Avg. loss: 0.379607\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.01, NNZs: 15, Bias: -1.294386, T: 487500, Avg. loss: 0.379251\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.03, NNZs: 15, Bias: -1.305805, T: 525000, Avg. loss: 0.378992\n",
      "Total training time: 0.06 seconds.\n",
      "Convergence after 14 epochs took 0.06 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=X_train, y=y_train) # fitting our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "EAfkVI6GyaRO",
    "outputId": "bc88f920-6531-4106-9b4c-4dabb6d72b47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.89007184,  0.63162363, -0.07594145,  0.63107107, -0.38434375,\n",
       "          0.93235243, -0.89573521, -0.07340522,  0.40591417,  0.4199991 ,\n",
       "          0.24722143,  0.05046199, -0.08877987,  0.54081652,  0.06643888]]),\n",
       " (1, 15),\n",
       " array([-1.30580538]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_, clf.coef_.shape, clf.intercept_\n",
    "#clf.coef_ will return the weights\n",
    "#clf.coef_.shape will return the shape of weights\n",
    "#clf.intercept_ will return the intercept term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-CcGTKgsMrY"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "```\n",
    "\n",
    "## <font color='red' size=5> Implementing Logistic Regression with L2 regularization Using SGD: without using sklearn </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zU2Y3-FQuJ3z"
   },
   "source": [
    "\n",
    "<br>\n",
    "<h2>We will follow the below steps</h2>\n",
    "\n",
    "1) Initialize the weight_vector and intercept term to zeros \n",
    "\n",
    "2) Create a loss function \n",
    "\n",
    " $log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$\n",
    "- for each epoch:\n",
    "\n",
    "    - for each batch of data points in train: (keep batch size=1)<br>\n",
    "\n",
    "        - calculate the gradient of loss function w.r.t each weight in weight vector <br>\n",
    "\n",
    "        $dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)})$ <br><br>\n",
    "\n",
    "        - Calculate the gradient of the intercept <br><br>\n",
    "           $ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t}))$<br><br>\n",
    "\n",
    "        - Update weights and intercept  <br><br>\n",
    "        $w^{(t+1)}← w^{(t)}+α(dw^{(t)}) $<br>\n",
    "\n",
    "        $b^{(t+1)}←b^{(t)}+α(db^{(t)}) $<br><br>\n",
    "    - calculate the log loss for train and test with the updated weights \n",
    "    - We can now compare the previous loss and the current loss, if it is not updating, then we can stop the training\n",
    "    - We will append this loss in a list ( this will be used to see how loss is changing for each epoch after the training is over )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZR_HgjgS_wKu"
   },
   "source": [
    "<font color='blue'>Initialize weights </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "GecwYV9fsKZ9"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(dim):\n",
    "    ''' In this function, we will initialize our weights and bias'''\n",
    "    #initialize the weights to zeros array of (1,dim) dimensions\n",
    "    #initialize bias to zero\n",
    "    w = np.zeros_like(X_train[0])\n",
    "    b = 0\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "A7I6uWBRsKc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "dim=X_train[0] \n",
    "w,b = initialize_weights(dim)\n",
    "print('w =',(w))\n",
    "print('b =',str(b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QN83oMWy_5rv"
   },
   "source": [
    "<font color='blue'>Compute sigmoid </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPv4NJuxABgs"
   },
   "source": [
    "$sigmoid(z)= 1/(1+exp(-z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "nAfmQF47_Sd6"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def sigmoid(z):\n",
    "    ''' In this function, we will return sigmoid of z'''\n",
    "    h = 1/(1+np.exp(-z))\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gS7JXbcrBOFF"
   },
   "source": [
    "<font color='blue'> Compute loss </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfEiS22zBVYy"
   },
   "source": [
    "$log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "VaFDgsp3sKi6"
   },
   "outputs": [],
   "source": [
    "def logloss(y_true,y_pred):\n",
    "    '''In this function, we will compute log loss '''\n",
    "    l = len(y_pred)\n",
    "    loss = 0\n",
    "    for ele in range(0,l):\n",
    "        temp_loss = (y_true[ele] * np.log10(y_pred[ele])) + ((1 - y_true[ele]) * np.log10(1 - y_pred[ele]))\n",
    "        loss += temp_loss\n",
    "    \n",
    "    loss = (-1 * loss)/l\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQabIadLCBAB"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to  'w' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTMxiYKaCQgd"
   },
   "source": [
    "$dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)}$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "NMVikyuFsKo5"
   },
   "outputs": [],
   "source": [
    "def gradient_dw(x,y,w,b,alpha,N):\n",
    "    '''In this function, we will compute the gardient w.r.to w '''\n",
    "    dw = (x * (y - sigmoid(np.dot(w,x) + b))) - ((alpha/N)*w)\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LE8g84_GI62n"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to 'b' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHvTYZzZJJ_N"
   },
   "source": [
    "$ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "0nUf2ft4EZp8"
   },
   "outputs": [],
   "source": [
    "def gradient_db(x,y,w,b):\n",
    "    db = y - sigmoid(np.dot(w,x) + b)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCK0jY_EOvyU"
   },
   "source": [
    "<font color='blue'> Implementing logistic regression</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "dmAdc5ejEZ25"
   },
   "outputs": [],
   "source": [
    "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):\n",
    "    ''' In this function, we will implement logistic regression'''\n",
    "    #Here eta0 is learning rate\n",
    "    #implement the code as follows\n",
    "    # initalize the weights (call the initialize_weights(X_train[0]) function)\n",
    "    # for every epoch\n",
    "        # for every data point(X_train,y_train)\n",
    "           #compute gradient w.r.to w (call the gradient_dw() function)\n",
    "           #compute gradient w.r.to b (call the gradient_db() function)\n",
    "           #update w, b\n",
    "        # predict the output of x_train[for all data points in X_train] using w,b\n",
    "        #compute the loss between predicted and actual values (call the loss function)\n",
    "        # store all the train loss values in a list\n",
    "        # predict the output of x_test[for all data points in X_test] using w,b\n",
    "        #compute the loss between predicted and actual values (call the loss function)\n",
    "        # store all the test loss values in a list\n",
    "        # you can also compare previous loss and current loss, if loss is not updating then stop the process and return w,b\n",
    "\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0,N):\n",
    "    w,b = initialize_weights(X_train[0])\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    for i in range(epochs):\n",
    "        y_train_pred = []\n",
    "        y_test_pred = []\n",
    "        for j in range(0,len(X_train)):\n",
    "            dw = gradient_dw(X_train[j],y_train[j],w,b,alpha,N)\n",
    "            db = gradient_db(X_train[j],y_train[j],w,b)\n",
    "            w = w + (eta0*dw)\n",
    "            b = b + (eta0*db)\n",
    "        for j in range(0,len(X_train)):\n",
    "            y_train_pred.append(sigmoid(np.dot(w,X_train[j]) + b))\n",
    "        loss = logloss(y_train,y_train_pred)\n",
    "        train_loss.append(loss)\n",
    "        for k in range(0,len(X_test)):\n",
    "            y_test_pred.append(sigmoid(np.dot(w,X_test[k]) + b))\n",
    "        loss = logloss(y_test,y_test_pred)\n",
    "        test_loss.append(loss)\n",
    "        print(f\"EPOCH{i+1}\\nTrain_loss = {train_loss[i]} , Test_loss = {test_loss[i]}\\n\")\n",
    "        print(\"=\"*15)\n",
    "        convergence = i+1\n",
    "        if len(test_loss) >= 2:\n",
    "            if test_loss[i] > test_loss[i-1]:\n",
    "                print(f\"\\nCONVERGENCE AFTER {i+1} EPOCHS , BEST_TEST_LOG_LOSS = {test_loss[i-1]}\")\n",
    "                return w,b,train_loss,test_loss,convergence\n",
    "                \n",
    "        \n",
    "    return w,b,train_loss,test_loss,convergence\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "sUquz7LFEZ6E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH1\n",
      "Train_loss = 0.20729781784140838 , Test_loss = 0.2072221978118188\n",
      "\n",
      "===============\n",
      "EPOCH2\n",
      "Train_loss = 0.18556210141426166 , Test_loss = 0.18565259434678275\n",
      "\n",
      "===============\n",
      "EPOCH3\n",
      "Train_loss = 0.17659652085620509 , Test_loss = 0.17682567720849304\n",
      "\n",
      "===============\n",
      "EPOCH4\n",
      "Train_loss = 0.17201289496451902 , Test_loss = 0.17235324848189568\n",
      "\n",
      "===============\n",
      "EPOCH5\n",
      "Train_loss = 0.16938000886115878 , Test_loss = 0.16981009840800468\n",
      "\n",
      "===============\n",
      "EPOCH6\n",
      "Train_loss = 0.16775336575455 , Test_loss = 0.16825663498220056\n",
      "\n",
      "===============\n",
      "EPOCH7\n",
      "Train_loss = 0.1666977629761566 , Test_loss = 0.16726128890692274\n",
      "\n",
      "===============\n",
      "EPOCH8\n",
      "Train_loss = 0.1659883750043287 , Test_loss = 0.16660192986644845\n",
      "\n",
      "===============\n",
      "EPOCH9\n",
      "Train_loss = 0.16549918227604976 , Test_loss = 0.16615457121757735\n",
      "\n",
      "===============\n",
      "EPOCH10\n",
      "Train_loss = 0.16515513945496196 , Test_loss = 0.16584572669386236\n",
      "\n",
      "===============\n",
      "EPOCH11\n",
      "Train_loss = 0.164909442960959 , Test_loss = 0.1656298054033339\n",
      "\n",
      "===============\n",
      "EPOCH12\n",
      "Train_loss = 0.1647318311394912 , Test_loss = 0.16547750036682654\n",
      "\n",
      "===============\n",
      "EPOCH13\n",
      "Train_loss = 0.16460216964645408 , Test_loss = 0.16536943679761335\n",
      "\n",
      "===============\n",
      "EPOCH14\n",
      "Train_loss = 0.16450674989278702 , Test_loss = 0.16529251625482547\n",
      "\n",
      "===============\n",
      "EPOCH15\n",
      "Train_loss = 0.16443606134127775 , Test_loss = 0.16523772268630538\n",
      "\n",
      "===============\n",
      "EPOCH16\n",
      "Train_loss = 0.1643834031232729 , Test_loss = 0.1651987592688669\n",
      "\n",
      "===============\n",
      "EPOCH17\n",
      "Train_loss = 0.16434399300380859 , Test_loss = 0.16517117642490298\n",
      "\n",
      "===============\n",
      "EPOCH18\n",
      "Train_loss = 0.16431438119164668 , Test_loss = 0.16515180015954944\n",
      "\n",
      "===============\n",
      "EPOCH19\n",
      "Train_loss = 0.1642920564949399 , Test_loss = 0.16513834939454075\n",
      "\n",
      "===============\n",
      "EPOCH20\n",
      "Train_loss = 0.16427517690890997 , Test_loss = 0.1651291752338442\n",
      "\n",
      "===============\n",
      "EPOCH21\n",
      "Train_loss = 0.16426238246016317 , Test_loss = 0.16512308060398706\n",
      "\n",
      "===============\n",
      "EPOCH22\n",
      "Train_loss = 0.16425266345784117 , Test_loss = 0.16511919387338203\n",
      "\n",
      "===============\n",
      "EPOCH23\n",
      "Train_loss = 0.16424526668148534 , Test_loss = 0.16511687931615676\n",
      "\n",
      "===============\n",
      "EPOCH24\n",
      "Train_loss = 0.16423962791724517 , Test_loss = 0.16511567308220965\n",
      "\n",
      "===============\n",
      "EPOCH25\n",
      "Train_loss = 0.16423532302395338 , Test_loss = 0.1651152370423451\n",
      "\n",
      "===============\n",
      "EPOCH26\n",
      "Train_loss = 0.16423203217373444 , Test_loss = 0.16511532529485728\n",
      "\n",
      "===============\n",
      "\n",
      "CONVERGENCE AFTER 26 EPOCHS , BEST_TEST_LOG_LOSS = 0.1651152370423451\n"
     ]
    }
   ],
   "source": [
    "alpha=0.0001\n",
    "eta0=0.0001\n",
    "N=len(X_train)\n",
    "epochs=50\n",
    "w,b,train_loss,test_loss,convergence=train(X_train,y_train,X_test,y_test,epochs,alpha,eta0,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight vector implemented from scratch = [-0.95666977  0.68450625 -0.10019437  0.67200277 -0.43276133  0.99396428\n",
      " -0.93472663 -0.07292547  0.43957276  0.46609465  0.26913868  0.0588015\n",
      " -0.09438729  0.56472222  0.0646547 ]\n",
      "\n",
      "Intercept implemented from scratch = -1.3568014565248212\n"
     ]
    }
   ],
   "source": [
    "print(f\"Weight vector implemented from scratch = {w}\\n\\nIntercept implemented from scratch = {b}\")\n",
    "# print(f\"TRAIN LOSS \\n{train_loss}\\n===========\\nTEST LOSS \\n{test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4Zf_wPARlwY"
   },
   "source": [
    "<font color='red'>Results</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3eF_VSPSH2z"
   },
   "source": [
    "Lets Compare our custom implementation with SGDClassifier's the weights and intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "nx8Rs9rfEZ1R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight vector from sklearn = [[-0.89007184  0.63162363 -0.07594145  0.63107107 -0.38434375  0.93235243\n",
      "  -0.89573521 -0.07340522  0.40591417  0.4199991   0.24722143  0.05046199\n",
      "  -0.08877987  0.54081652  0.06643888]]\n",
      "\n",
      "Intercept from sklearn = [-1.30580538]\n"
     ]
    }
   ],
   "source": [
    "# these are the results we got after we implemented sgd and found the optimal weights and intercept\n",
    "print(f\"Weight vector from sklearn = {clf.coef_}\\n\\nIntercept from sklearn = {clf.intercept_}\")\n",
    "\n",
    "# clf.coef_, clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PrettyTable in c:\\users\\shafiq\\anaconda3\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\shafiq\\anaconda3\\lib\\site-packages (from PrettyTable) (0.2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+---------------------------------------------------------------------------+\n",
      "|                Field 1                |                                  Field 2                                  |\n",
      "+---------------------------------------+---------------------------------------------------------------------------+\n",
      "|  weights_from_scratch_implementation  |  [-0.95666977  0.68450625 -0.10019437  0.67200277 -0.43276133  0.99396428 |\n",
      "|                                       |   -0.93472663 -0.07292547  0.43957276  0.46609465  0.26913868  0.0588015  |\n",
      "|                                       |                    -0.09438729  0.56472222  0.0646547 ]                   |\n",
      "|          weights_from_sklearn         | [[-0.89007184  0.63162363 -0.07594145  0.63107107 -0.38434375  0.93235243 |\n",
      "|                                       |   -0.89573521 -0.07340522  0.40591417  0.4199991   0.24722143  0.05046199 |\n",
      "|                                       |                    -0.08877987  0.54081652  0.06643888]]                  |\n",
      "| intercept_from_scratch_implementation |                            -1.3568014565248212                            |\n",
      "|         intercept_from_sklearn        |                               [-1.30580538]                               |\n",
      "+---------------------------------------+---------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "weights_from_scratch_implementation=['weights_from_scratch_implementation',w]\n",
    "intercept_from_scratch_implementation=['intercept_from_scratch_implementation',b]\n",
    "weights_from_sklearn=['weights_from_sklearn',clf.coef_]\n",
    "intercept_from_sklearn=['intercept_from_sklearn',clf.intercept_]\n",
    "\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "from prettytable import MSWORD_FRIENDLY,FRAME\n",
    "x.set_style(MSWORD_FRIENDLY)\n",
    "x = PrettyTable()\n",
    "x.hrules = FRAME\n",
    "# x = PrettyTable( padding_width=5)\n",
    "x.add_row(weights_from_scratch_implementation)\n",
    "x.add_row(weights_from_sklearn)\n",
    "x.add_row(intercept_from_scratch_implementation)\n",
    "x.add_row(intercept_from_sklearn)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>We can see that the Scratch Implementation VS Sklearn results are very close to each other. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "230YbSgNSUrQ"
   },
   "source": [
    "<font color='blue'>Plotting Epochs VS Train and Test loss </font>\n",
    "\n",
    "* Epoch number on X-axis\n",
    "* Log_loss on Y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "1O6GrRt7UeCJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "epoch = [i for i in range(1,convergence+1,1)]\n",
    "\n",
    "plt.plot(epoch,train_loss , label='train_log_loss')\n",
    "plt.plot(epoch,test_loss, label='test_log_loss')\n",
    "plt.xlabel(\"epoch number\")\n",
    "plt.ylabel(\"log loss\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Calculating accuracy </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "FUN8puFoEZtU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy = 0.9514666666666667 \n",
      "Test accuracy = 0.9488\n"
     ]
    }
   ],
   "source": [
    "def pred(w,b, X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        z=np.dot(w,X[i])+b\n",
    "        if sigmoid(z) >= 0.5: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n",
    "            predict.append(1)\n",
    "        else:\n",
    "            predict.append(0)\n",
    "    return np.array(predict)\n",
    "accuracy_train = (1-np.sum(y_train - pred(w,b,X_train))/len(X_train))\n",
    "accuracy_test = (1-np.sum(y_test  - pred(w,b,X_test))/len(X_test))\n",
    "\n",
    "print(f\"Train accuracy = {accuracy_train} \\nTest accuracy = {accuracy_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='Violet'>Conclusion : We have successfully implemented logistic regression with SGD from scratch without using Sklearn</font>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
